{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "filename = '/content/Accounts.csv'\n",
        "dataset = pd.read_csv(filename)\n",
        "\n",
        "# NaN values\n",
        "cols_with_nan = dataset.columns[dataset.isna().any()].tolist()\n",
        "for col in cols_with_nan:\n",
        "    if dataset[col].isna().any():\n",
        "        dataset[col] = dataset.groupby('Class')[col].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "    else:\n",
        "        print(f\"\")\n",
        "\n",
        "X = dataset.iloc[:, 1:46].values\n",
        "y = dataset.iloc[:, 47].values\n",
        "\n",
        "# features names\n",
        "feature_names = dataset.columns[1:46]\n",
        "\n",
        "# Hyperparameters for SVM linear\n",
        "svm_parameters = {'C': [10**i for i in range(-3, 0, 10)]}\n",
        "\n",
        "# Split the data into training, testing, and validation sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Nested cross-validation strategy\n",
        "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Performance metric lists\n",
        "svm_accuracy_scores = []\n",
        "svm_precision_scores = []\n",
        "svm_recall_scores = []\n",
        "svm_f1_scores = []\n",
        "\n",
        "# Initialize a list\n",
        "feature_importance_scores = []\n",
        "\n",
        "# Perform nested cross-validation\n",
        "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
        "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # SVM linear\n",
        "    svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "    svm_grid_search = GridSearchCV(svm_classifier, svm_parameters, cv=inner_cv)\n",
        "    svm_grid_search.fit(X_train_fold, y_train_fold)\n",
        "    svm_best_params = svm_grid_search.best_params_\n",
        "    svm_best_model = SVC(kernel='linear', random_state=42, **svm_best_params)\n",
        "\n",
        "    # Ablation Study\n",
        "    ablation_scores = []\n",
        "    for feature_index in range(X_train_fold.shape[1]):\n",
        "        ablated_X_train = np.delete(X_train_fold, feature_index, axis=1)\n",
        "        ablated_X_test = np.delete(X_test_fold, feature_index, axis=1)\n",
        "        svm_best_model.fit(ablated_X_train, y_train_fold)\n",
        "        svm_predictions = svm_best_model.predict(ablated_X_test)\n",
        "        ablation_scores.append(accuracy_score(y_test_fold, svm_predictions))\n",
        "\n",
        "    # Store feature importance scores\n",
        "    feature_importance_scores.append(ablation_scores)\n",
        "\n",
        "    # Evaluate SVM linear performance metrics\n",
        "    svm_accuracy_scores.append(np.mean(ablation_scores))\n",
        "    svm_precision_scores.append(precision_score(y_test_fold, svm_predictions))\n",
        "    svm_recall_scores.append(recall_score(y_test_fold, svm_predictions))\n",
        "    svm_f1_scores.append(f1_score(y_test_fold, svm_predictions))\n",
        "\n",
        "# Calculate average performance scores\n",
        "svm_average_accuracy = np.mean(svm_accuracy_scores)\n",
        "svm_average_precision = np.mean(svm_precision_scores)\n",
        "svm_average_recall = np.mean(svm_recall_scores)\n",
        "svm_average_f1 = np.mean(svm_f1_scores)\n",
        "svm_std_accuracy = np.std(svm_accuracy_scores)\n",
        "svm_std_precision = np.std(svm_precision_scores)\n",
        "svm_std_recall = np.std(svm_recall_scores)\n",
        "svm_std_f1 = np.std(svm_f1_scores)\n",
        "\n",
        "# Print the average performance scores\n",
        "print(\"\\nSVM Linear:\")\n",
        "print(\"Average Accuracy:\", round(svm_average_accuracy,4))\n",
        "print(\"Standard Deviation Accuracy:\",round(svm_std_accuracy,4))\n",
        "print(\"Average Precision:\", round(svm_average_precision,4))\n",
        "print(\"Standard Deviation Precision:\", round(svm_std_precision,4))\n",
        "print(\"Average Recall:\", round(svm_average_recall,4))\n",
        "print(\"Standard Deviation Recall:\", round(svm_std_recall,4))\n",
        "print(\"Average F1-score:\", round(svm_average_f1,4))\n",
        "print(\"Standard Deviation F1-score:\", round(svm_std_f1,4))\n"
      ],
      "metadata": {
        "id": "ssU7Oh_4qY2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dcb280c-30da-48ba-806e-edf3ea7e53cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SVM Linear:\n",
            "Average Accuracy: 0.9767\n",
            "Standard Deviation Accuracy: 0.0061\n",
            "Average Precision: 0.9838\n",
            "Standard Deviation Precision: 0.0077\n",
            "Average Recall: 0.9685\n",
            "Standard Deviation Recall: 0.0112\n",
            "Average F1-score: 0.976\n",
            "Standard Deviation F1-score: 0.007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best-performing features\n",
        "best_feature_indices = np.argsort(np.mean(feature_importance_scores, axis=0))[::-1]\n",
        "\n",
        "num_features_to_display = 10\n",
        "X_best_features = X[:, best_feature_indices[:num_features_to_display]]\n",
        "\n",
        "# Calculate and print feature importance with feature names\n",
        "average_importance_scores = np.mean(feature_importance_scores, axis=0)\n",
        "sorted_indices = np.argsort(average_importance_scores)[::-1]\n",
        "\n",
        "print(\"\\nTop Feature Importance (Ablation):\")\n",
        "for i in range(num_features_to_display):\n",
        "    feature_index = sorted_indices[i]\n",
        "    importance_score = round(average_importance_scores[feature_index], 5)\n",
        "    print(f\"{feature_names[feature_index]}: {importance_score}\")\n",
        "\n",
        "#................................................................"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1Gu80NfrC3d",
        "outputId": "2d623472-b21b-4c5e-f8f6-4422f43d0eba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Feature Importance (Ablation):\n",
            "Gunning Fog Mean: 0.97775\n",
            " Count Blanks STD: 0.9775\n",
            "Count Blanks Mean: 0.9775\n",
            "Automated Readability Index Mean: 0.9775\n",
            " Num Lowercase Chars STD: 0.97725\n",
            "Total Characters STD: 0.97725\n",
            "Average Words Length Mean: 0.97725\n",
            "Dale Chall Readability Mean: 0.97725\n",
            " Dale Chall Readability STD: 0.97725\n",
            "Linsear Write STD: 0.97725\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
