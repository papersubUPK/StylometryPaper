{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nKKljMiStor8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4a30e4-b138-4ff9-91d0-b73a488f6229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Decision Tree\n",
            "Average Accuracy: 0.9852\n",
            "Standard Deviation Accuracy: 0.0\n",
            "Average Precision: 0.992\n",
            "Standard Deviation Precision: 0.0\n",
            "Average Recall: 0.9763\n",
            "Standard Deviation Recall: 0.0\n",
            "Average F1-score: 0.9841\n",
            "Standard Deviation F1-score: 0.0\n"
          ]
        }
      ],
      "source": [
        "#Decision Tree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "filename = '/content/Accounts.csv'\n",
        "dataset = pd.read_csv(filename)\n",
        "\n",
        "# Remove NaN values\n",
        "cols_with_nan = dataset.columns[dataset.isna().any()].tolist()\n",
        "for col in cols_with_nan:\n",
        "    if dataset[col].isna().any():\n",
        "        dataset[col] = dataset.groupby('Class')[col].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "    else:\n",
        "        print(f\"\")\n",
        "\n",
        "X = dataset.iloc[:, 1:46].values\n",
        "y = dataset.iloc[:, 47].values\n",
        "\n",
        "feature_names = dataset.columns[1:46]\n",
        "\n",
        "# Hyperparameters for Decision Tree\n",
        "dt_parameters = {'max_depth': range(1, 5), 'criterion': ['gini', 'entropy']}\n",
        "\n",
        "# Split the data into training, testing, and validation sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Nested cross-validation strategy\n",
        "outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "inner_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize performance metric lists\n",
        "dt_accuracy_scores = []\n",
        "dt_precision_scores = []\n",
        "dt_recall_scores = []\n",
        "dt_f1_scores = []\n",
        "\n",
        "# Store feature importance scores\n",
        "feature_importance_scores = []\n",
        "\n",
        "# Perform nested cross-validation\n",
        "for train_index, test_index in outer_cv.split(X_train, y_train):\n",
        "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
        "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
        "\n",
        "    # Decision Tree\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "    dt_grid_search = GridSearchCV(dt_classifier, dt_parameters, cv=inner_cv)\n",
        "    dt_grid_search.fit(X_train, y_train)\n",
        "    dt_best_params = dt_grid_search.best_params_\n",
        "    dt_best_model = DecisionTreeClassifier(random_state=42, **dt_best_params)\n",
        "\n",
        "    # Ablation Study\n",
        "    ablation_scores = []\n",
        "    for feature_index in range(X_train.shape[1]):\n",
        "        ablated_X_train = np.delete(X_train, feature_index, axis=1)\n",
        "        ablated_X_test = np.delete(X_test, feature_index, axis=1)\n",
        "        dt_best_model.fit(ablated_X_train, y_train)\n",
        "        dt_predictions = dt_best_model.predict(ablated_X_test)\n",
        "        ablation_scores.append(accuracy_score(y_test, dt_predictions))\n",
        "\n",
        "    # Store feature importance scores\n",
        "    feature_importance_scores.append(ablation_scores)\n",
        "\n",
        "    # Evaluate performance metrics\n",
        "    dt_accuracy_scores.append(np.mean(ablation_scores))\n",
        "    dt_precision_scores.append(precision_score(y_test, dt_predictions))\n",
        "    dt_recall_scores.append(recall_score(y_test, dt_predictions))\n",
        "    dt_f1_scores.append(f1_score(y_test, dt_predictions))\n",
        "\n",
        "# Calculate average performance scores\n",
        "dt_average_accuracy = np.mean(dt_accuracy_scores)\n",
        "dt_average_precision = np.mean(dt_precision_scores)\n",
        "dt_average_recall = np.mean(dt_recall_scores)\n",
        "dt_average_f1 = np.mean(dt_f1_scores)\n",
        "dt_std_accuracy= np.std(dt_accuracy_scores)\n",
        "dt_std_precision = np.std(dt_precision_scores)\n",
        "dt_std_recall = np.std(dt_recall_scores)\n",
        "dt_std_f1 = np.std(dt_f1_scores)\n",
        "\n",
        "# Print the average scores\n",
        "print(\"\\nDecision Tree\")\n",
        "print(\"Average Accuracy:\", round(dt_average_accuracy,4))\n",
        "print(\"Standard Deviation Accuracy:\",round(dt_std_accuracy,4))\n",
        "print(\"Average Precision:\", round(dt_average_precision,4))\n",
        "print(\"Standard Deviation Precision:\", round(dt_std_precision,4))\n",
        "print(\"Average Recall:\", round(dt_average_recall,4))\n",
        "print(\"Standard Deviation Recall:\", round(dt_std_recall,4))\n",
        "print(\"Average F1-score:\", round(dt_average_f1,4))\n",
        "print(\"Standard Deviation F1-score:\", round(dt_std_f1,4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the best-performing features\n",
        "best_feature_indices = np.argsort(np.mean(feature_importance_scores, axis=0))[::-1]\n",
        "num_features_to_display = 10\n",
        "\n",
        "X_best_features = X[:, best_feature_indices[:num_features_to_display]]\n",
        "\n",
        "average_importance_scores = np.mean(feature_importance_scores, axis=0)\n",
        "sorted_indices = np.argsort(average_importance_scores)[::-1]\n",
        "\n",
        "print(\"\\nTop Feature Importance With Scores:\")\n",
        "for i in range(num_features_to_display):\n",
        "    feature_index = sorted_indices[i]\n",
        "    importance_score = round(average_importance_scores[feature_index], 5)\n",
        "    print(f\"{feature_names[feature_index]}: {importance_score}\")"
      ],
      "metadata": {
        "id": "WGiGHi7S5EZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c66212-fc8d-49c6-acc5-1b510480d94c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Feature Importance With Scores:\n",
            "linsear_write: 0.86933\n",
            "ari: 0.86933\n",
            "gunning_fog: 0.86933\n",
            "coleman_liau_index: 0.86933\n",
            "count_special_characters: 0.86933\n",
            "count_numbers: 0.86933\n",
            "num_lowercase_words: 0.86933\n",
            "total_characters: 0.86933\n",
            "num_uppercase_chars: 0.86867\n",
            "count_unique_words: 0.86867\n"
          ]
        }
      ]
    }
  ]
}
